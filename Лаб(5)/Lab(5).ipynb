{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "text1 = 'Из графика распределения вероятности сделки видно, что большинство позиций имеют крайне низкую вероятность сделки, т.е. около 78%, при этом очень немногие значения имеют вероятность сделки 0,7 и выше.'\n",
        "text2 = 'Рядом с вероятностью сделки 1,0 наблюдается очень маленькая башня, что указывает на то, что в наборе данных есть некоторые элементы с очень высоким значением вероятности сделки.'\n",
        "text3 = 'Такие категории, как растения, планшеты и электронные книги, имеют относительно меньшее количество рекламы, представленной на Авито. Они могут плохо работать на Авито.'"
      ],
      "metadata": {
        "id": "PYaMa0FYQnSB"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "wpInsV2fQnVR",
        "outputId": "e5abf7c6-c6b0-4e0e-eac2-b94e625232d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import tokenize\n",
        "dir(tokenize)[:18]"
      ],
      "metadata": {
        "id": "1oyWIJumQnYQ",
        "outputId": "04432021-f1ed-4163-dc1d-0ed5847c13b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BlanklineTokenizer',\n",
              " 'LineTokenizer',\n",
              " 'MWETokenizer',\n",
              " 'PunktSentenceTokenizer',\n",
              " 'RegexpTokenizer',\n",
              " 'ReppTokenizer',\n",
              " 'SExprTokenizer',\n",
              " 'SpaceTokenizer',\n",
              " 'StanfordSegmenter',\n",
              " 'TabTokenizer',\n",
              " 'TextTilingTokenizer',\n",
              " 'ToktokTokenizer',\n",
              " 'TreebankWordTokenizer',\n",
              " 'TweetTokenizer',\n",
              " 'WhitespaceTokenizer',\n",
              " 'WordPunctTokenizer',\n",
              " '__builtins__',\n",
              " '__cached__']"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_tk_1 = nltk.WordPunctTokenizer()\n",
        "nltk_tk_1.tokenize(text1)"
      ],
      "metadata": {
        "id": "2ZUIAcmYQnbf",
        "outputId": "374d1ede-72f4-4bd8-fff9-820e822bdcd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Из',\n",
              " 'графика',\n",
              " 'распределения',\n",
              " 'вероятности',\n",
              " 'сделки',\n",
              " 'видно',\n",
              " ',',\n",
              " 'что',\n",
              " 'большинство',\n",
              " 'позиций',\n",
              " 'имеют',\n",
              " 'крайне',\n",
              " 'низкую',\n",
              " 'вероятность',\n",
              " 'сделки',\n",
              " ',',\n",
              " 'т',\n",
              " '.',\n",
              " 'е',\n",
              " '.',\n",
              " 'около',\n",
              " '78',\n",
              " '%,',\n",
              " 'при',\n",
              " 'этом',\n",
              " 'очень',\n",
              " 'немногие',\n",
              " 'значения',\n",
              " 'имеют',\n",
              " 'вероятность',\n",
              " 'сделки',\n",
              " '0',\n",
              " ',',\n",
              " '7',\n",
              " 'и',\n",
              " 'выше',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Токенизация по предложениям\n",
        "nltk_tk_sents = nltk.tokenize.sent_tokenize(text1)\n",
        "print(len(nltk_tk_sents))\n",
        "nltk_tk_sents"
      ],
      "metadata": {
        "id": "_twCz_g7Qnex",
        "outputId": "f8ebb071-b228-4285-85c7-0ec5eea742a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Из графика распределения вероятности сделки видно, что большинство позиций имеют крайне низкую вероятность сделки, т.е.',\n",
              " 'около 78%, при этом очень немногие значения имеют вероятность сделки 0,7 и выше.']"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from navec import Navec\n",
        "from slovnet import Morph"
      ],
      "metadata": {
        "id": "HVkOPcLAQniE"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install natasha"
      ],
      "metadata": {
        "id": "PbAEX77xSu4T",
        "outputId": "baba6ca5-56d0-49b7-c1b5-d737ddc806b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: natasha in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: slovnet>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from natasha) (0.5.0)\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (from natasha) (0.9.1)\n",
            "Requirement already satisfied: razdel>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from natasha) (0.5.0)\n",
            "Requirement already satisfied: ipymarkup>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from natasha) (0.9.0)\n",
            "Requirement already satisfied: yargy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from natasha) (0.15.0)\n",
            "Requirement already satisfied: navec>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from natasha) (0.10.0)\n",
            "Requirement already satisfied: intervaltree>=3 in /usr/local/lib/python3.7/dist-packages (from ipymarkup>=0.8.0->natasha) (3.1.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from navec>=0.9.0->natasha) (1.21.6)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->natasha) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->natasha) (0.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "id": "tn6by-XDQnlr",
        "outputId": "0bb31a75-267b-4c01-e903-0ae410ef1b04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install slovnet"
      ],
      "metadata": {
        "id": "1iMLlxVAUD3x",
        "outputId": "60e8d794-8509-415f-d1dd-d27aa90cd25f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: slovnet in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: razdel in /usr/local/lib/python3.7/dist-packages (from slovnet) (0.5.0)\n",
            "Requirement already satisfied: navec in /usr/local/lib/python3.7/dist-packages (from slovnet) (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from slovnet) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Файл необходимо скачать по ссылке https://github.com/natasha/slovnet#downloads\n",
        "n_morph = Morph.load('/content/drive/MyDrive/slovnet_morph_news_v1.tar', batch_size=4)"
      ],
      "metadata": {
        "id": "q2ycfyK-UdsQ"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from navec import Navec\n",
        "from slovnet import Morph"
      ],
      "metadata": {
        "id": "JxUjtMO2YgxA"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_pos(markup):\n",
        "    for token in markup.tokens:\n",
        "        print('{} - {}'.format(token.text, token.tag))"
      ],
      "metadata": {
        "id": "0TB6497bYl73"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from natasha import Doc, Segmenter, NewsEmbedding, NewsMorphTagger, MorphVocab"
      ],
      "metadata": {
        "id": "do3Tom0JZFpn"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def n_lemmatize(text):\n",
        "    emb = NewsEmbedding()\n",
        "    morph_tagger = NewsMorphTagger(emb)\n",
        "    segmenter = Segmenter()\n",
        "    morph_vocab = MorphVocab()\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    for token in doc.tokens:\n",
        "        token.lemmatize(morph_vocab)\n",
        "    return doc"
      ],
      "metadata": {
        "id": "Z_Je1v9iZJNP"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_doc1 = n_lemmatize(text1)\n",
        "{_.text: _.lemma for _ in n_doc1.tokens}"
      ],
      "metadata": {
        "id": "Udy_dPhUZL-A",
        "outputId": "7be7a041-8e2a-45b5-a1d9-6f32bc30a7a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'%': '%',\n",
              " ',': ',',\n",
              " '.': '.',\n",
              " '0,7': '0,7',\n",
              " '78': '78',\n",
              " 'Из': 'из',\n",
              " 'большинство': 'большинство',\n",
              " 'вероятности': 'вероятность',\n",
              " 'вероятность': 'вероятность',\n",
              " 'видно': 'видно',\n",
              " 'выше': 'выше',\n",
              " 'графика': 'график',\n",
              " 'е': 'быть',\n",
              " 'значения': 'значение',\n",
              " 'и': 'и',\n",
              " 'имеют': 'иметь',\n",
              " 'крайне': 'крайне',\n",
              " 'немногие': 'немногий',\n",
              " 'низкую': 'низкий',\n",
              " 'около': 'около',\n",
              " 'очень': 'очень',\n",
              " 'позиций': 'позиция',\n",
              " 'при': 'при',\n",
              " 'распределения': 'распределение',\n",
              " 'сделки': 'сделка',\n",
              " 'т': 'т',\n",
              " 'что': 'что',\n",
              " 'этом': 'это'}"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_doc2 = n_lemmatize(text2)\n",
        "{_.text: _.lemma for _ in n_doc2.tokens}"
      ],
      "metadata": {
        "id": "eRG8xb_mZWk_",
        "outputId": "0288ce81-f94b-4ec7-ac30-c29742cea429",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{',': ',',\n",
              " '.': '.',\n",
              " '1,0': '1,0',\n",
              " 'Рядом': 'рядом',\n",
              " 'башня': 'башня',\n",
              " 'в': 'в',\n",
              " 'вероятности': 'вероятность',\n",
              " 'вероятностью': 'вероятность',\n",
              " 'высоким': 'высокий',\n",
              " 'данных': 'данные',\n",
              " 'есть': 'быть',\n",
              " 'значением': 'значение',\n",
              " 'маленькая': 'маленький',\n",
              " 'на': 'на',\n",
              " 'наблюдается': 'наблюдаться',\n",
              " 'наборе': 'набор',\n",
              " 'некоторые': 'некоторый',\n",
              " 'очень': 'очень',\n",
              " 'с': 'с',\n",
              " 'сделки': 'сделка',\n",
              " 'то': 'тот',\n",
              " 'указывает': 'указывать',\n",
              " 'что': 'что',\n",
              " 'элементы': 'элемент'}"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_doc3 = n_lemmatize(text3)\n",
        "{_.text: _.lemma for _ in n_doc3.tokens}"
      ],
      "metadata": {
        "id": "eUaI6kz3ZcAH",
        "outputId": "090f80d6-757c-4e54-f44d-4d0f5c80a553",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{',': ',',\n",
              " '.': '.',\n",
              " 'Авито': 'авить',\n",
              " 'Они': 'они',\n",
              " 'Такие': 'такой',\n",
              " 'и': 'и',\n",
              " 'имеют': 'иметь',\n",
              " 'как': 'как',\n",
              " 'категории': 'категория',\n",
              " 'книги': 'книга',\n",
              " 'количество': 'количество',\n",
              " 'меньшее': 'малый',\n",
              " 'могут': 'мочь',\n",
              " 'на': 'на',\n",
              " 'относительно': 'относительно',\n",
              " 'планшеты': 'планшет',\n",
              " 'плохо': 'плохо',\n",
              " 'представленной': 'представить',\n",
              " 'работать': 'работать',\n",
              " 'растения': 'растение',\n",
              " 'рекламы': 'реклама',\n",
              " 'электронные': 'электронный'}"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from slovnet import NER\n",
        "from ipymarkup import show_span_ascii_markup as show_markup"
      ],
      "metadata": {
        "id": "LDPwJolXZgMn"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner = NER.load('/content/drive/MyDrive/slovnet_ner_news_v1 (1).tar')"
      ],
      "metadata": {
        "id": "uZoVFE1XZtLo"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from natasha import NewsSyntaxParser"
      ],
      "metadata": {
        "id": "p9FqyIP-Z_um"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb = NewsEmbedding()\n",
        "syntax_parser = NewsSyntaxParser(emb)"
      ],
      "metadata": {
        "id": "g0ogZaqGZ_xv"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_doc1.parse_syntax(syntax_parser)\n",
        "n_doc1.sents[0].syntax.print()"
      ],
      "metadata": {
        "id": "LcP4W7CzZ_1V",
        "outputId": "504daf78-2ed1-41f0-9e5a-50eccedbda0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  ┌► Из            case\n",
            "              ┌►┌─└─ графика       obl\n",
            "              │ └►┌─ распределения nmod\n",
            "              │ ┌─└► вероятности   nmod\n",
            "              │ └──► сделки        nmod\n",
            "┌─┌─────┌─┌───└───── видно         \n",
            "│ │     │ │ ┌──────► ,             punct\n",
            "│ │     │ │ │ ┌────► что           mark\n",
            "│ │     │ │ │ │ ┌►┌─ большинство   nsubj\n",
            "│ │     │ │ │ │ │ └► позиций       nmod\n",
            "│ │   ┌─│ └►└─└─└─── имеют         ccomp\n",
            "│ │   │ │   │     ┌► крайне        advmod\n",
            "│ │   │ │   │   ┌►└─ низкую        amod\n",
            "│ │   │ │   └──►└─┌─ вероятность   obj\n",
            "│ │   │ │         └► сделки        nmod\n",
            "│ │   │ │         ┌► ,             punct\n",
            "│ │ ┌►│ │   ┌─┌─┌─└─ т             mark\n",
            "│ │ │ │ │   │ │ └──► .             fixed\n",
            "│ │ │ │ │   │ └────► е             fixed\n",
            "│ │ │ │ │   └──────► .             punct\n",
            "│ │ │ │ │       ┌──► около         case\n",
            "│ │ │ │ │       │ ┌► 78            nummod\n",
            "│ │ │ │ └──────►└─└─ %             ccomp\n",
            "│ │ │ │     ┌──────► ,             punct\n",
            "│ │ │ │     │     ┌► при           case\n",
            "│ │ │ │     │ ┌──►└─ этом          obl\n",
            "│ │ │ │     │ │   ┌► очень         advmod\n",
            "│ │ │ │     │ │ ┌►└─ немногие      amod\n",
            "│ │ │ └────►│ │ └─── значения      obj\n",
            "│ └►│       └─└───┌─ имеют         ccomp\n",
            "│   │           ┌─└► вероятность   obj\n",
            "│   │           └►┌─ сделки        nmod\n",
            "│   │           ┌─└► 0,7           nummod\n",
            "│   │           │ ┌► и             cc\n",
            "│   └───────────└►└─ выше          conj\n",
            "└──────────────────► .             punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_doc3.parse_syntax(syntax_parser)\n",
        "n_doc3.sents[0].syntax.print()"
      ],
      "metadata": {
        "id": "KJv5_3ALZ_3_",
        "outputId": "ef5516ab-9ffd-4452-ff8f-75d19a883f9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            ┌► Такие          det\n",
            "┌──────────►└─ категории      nsubj\n",
            "│         ┌──► ,              punct\n",
            "│         │ ┌► как            case\n",
            "│ ┌►┌─┌─┌─└─└─ растения       obl\n",
            "│ │ │ │ │   ┌► ,              punct\n",
            "│ │ │ │ └──►└─ планшеты       conj\n",
            "│ │ │ │   ┌──► и              cc\n",
            "│ │ │ │   │ ┌► электронные    amod\n",
            "│ │ │ └──►└─└─ книги          conj\n",
            "│ │ └────────► ,              punct\n",
            "└─└─────┌───── имеют          \n",
            "│       │   ┌► относительно   advmod\n",
            "│       │ ┌►└─ меньшее        amod\n",
            "│       └►└─┌─ количество     obj\n",
            "│         ┌─└► рекламы        nmod\n",
            "│         │ ┌► ,              punct\n",
            "│       ┌─└►└─ представленной acl\n",
            "│       │   ┌► на             case\n",
            "│       └──►└─ Авито          obl\n",
            "└────────────► .              punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_doc2.parse_syntax(syntax_parser)\n",
        "n_doc2.sents[0].syntax.print()"
      ],
      "metadata": {
        "id": "JRc7wM5yZ_7L",
        "outputId": "328257e5-390f-4925-cdf5-5706dfbee294",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              ┌────► Рядом        advmod\n",
            "              │   ┌► с            case\n",
            "              │ ┌─└─ вероятностью \n",
            "              │ └►┌─ сделки       nmod\n",
            "              │   └► 1,0          nmod\n",
            "┌─┌─┌─┌─┌─┌►┌─└───── наблюдается  acl\n",
            "│ │ │ │ │ │ │ │   ┌► очень        advmod\n",
            "│ │ │ │ │ │ │ │ ┌►└─ маленькая    amod\n",
            "│ │ │ │ │ │ │ └►└─── башня        nsubj\n",
            "│ │ │ │ │ │ │   ┌──► ,            punct\n",
            "│ │ │ │ │ │ │   │ ┌► что          nsubj\n",
            "│ │ │ │ │ │ └►┌─└─└─ указывает    parataxis\n",
            "│ │ │ │ │ │   │   ┌► на           case\n",
            "│ │ │ │ │ └─┌─└──►└─ то           obl\n",
            "│ │ │ │ └──►│        ,            punct\n",
            "│ │ │ └────►│        что          mark\n",
            "│ │ │       │     ┌► в            case\n",
            "│ │ └──────►│   ┌─└─ наборе       obl\n",
            "│ │         │   └──► данных       nmod\n",
            "│ │       ┌─└──────► есть         acl\n",
            "│ │       │       ┌► некоторые    det\n",
            "│ └──────►│       └─ элементы     nsubj\n",
            "│         │   ┌────► с            case\n",
            "│         │   │   ┌► очень        advmod\n",
            "│         │   │ ┌►└─ высоким      amod\n",
            "└────────►│   └─└─── значением    obl\n",
            "          │     ┌─┌► вероятности  amod\n",
            "          │     └►└─ сделки       nmod\n",
            "          └────────► .            punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import word2vec"
      ],
      "metadata": {
        "id": "lHRhm01LbD2J"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score \n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "\n",
        "%matplotlib inline \n",
        "sns.set(style=\"ticks\")"
      ],
      "metadata": {
        "id": "5YMvbnAxbHb2"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\"rec.motorcycles\", \"rec.sport.baseball\", \"sci.electronics\",\"sci.med\"]\n",
        "newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
        "data = newsgroups['data']"
      ],
      "metadata": {
        "id": "FVpzuqlYbKXQ"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_score_for_classes(\n",
        "    y_true: np.ndarray, \n",
        "    y_pred: np.ndarray) -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    Вычисление метрики accuracy для каждого класса\n",
        "    y_true - истинные значения классов\n",
        "    y_pred - предсказанные значения классов\n",
        "    Возвращает словарь: ключ - метка класса, \n",
        "    значение - Accuracy для данного класса\n",
        "    \"\"\"\n",
        "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
        "    d = {'t': y_true, 'p': y_pred}\n",
        "    df = pd.DataFrame(data=d)\n",
        "    # Метки классов\n",
        "    classes = np.unique(y_true)\n",
        "    # Результирующий словарь\n",
        "    res = dict()\n",
        "    # Перебор меток классов\n",
        "    for c in classes:\n",
        "        # отфильтруем данные, которые соответствуют \n",
        "        # текущей метке класса в истинных значениях\n",
        "        temp_data_flt = df[df['t']==c]\n",
        "        # расчет accuracy для заданной метки класса\n",
        "        temp_acc = accuracy_score(\n",
        "            temp_data_flt['t'].values, \n",
        "            temp_data_flt['p'].values)\n",
        "        # сохранение результата в словарь\n",
        "        res[c] = temp_acc\n",
        "    return res\n",
        "\n",
        "def print_accuracy_score_for_classes(\n",
        "    y_true: np.ndarray, \n",
        "    y_pred: np.ndarray):\n",
        "    \"\"\"\n",
        "    Вывод метрики accuracy для каждого класса\n",
        "    \"\"\"\n",
        "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
        "    if len(accs)>0:\n",
        "        print('Метка \\t Accuracy')\n",
        "    for i in accs:\n",
        "        print('{} \\t {}'.format(i, accs[i]))"
      ],
      "metadata": {
        "id": "jfKmaQSTbpuC"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabVect = CountVectorizer()\n",
        "vocabVect.fit(data)\n",
        "corpusVocab = vocabVect.vocabulary_\n",
        "print('Количество сформированных признаков - {}'.format(len(corpusVocab)))"
      ],
      "metadata": {
        "id": "j4tGWmzwbup1",
        "outputId": "c95fb41a-f91b-4500-af2c-6ab03d3657f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество сформированных признаков - 33448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in list(corpusVocab)[1:10]:\n",
        "    print('{}={}'.format(i, corpusVocab[i]))"
      ],
      "metadata": {
        "id": "hXbsdmq9byZn",
        "outputId": "9f73a5ba-92d7-4242-bfbb-5f6bb5c7deb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nrmendel=22213\n",
            "unix=31462\n",
            "amherst=5287\n",
            "edu=12444\n",
            "nathaniel=21624\n",
            "mendell=20477\n",
            "subject=29220\n",
            "re=25369\n",
            "bike=6898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = vocabVect.transform(data)\n",
        "test_features"
      ],
      "metadata": {
        "id": "_KNI75Zmb1eo",
        "outputId": "3e60630b-54af-417b-b9f0-6e74b1849935",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2380x33448 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 335176 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_features.todense()"
      ],
      "metadata": {
        "id": "w2CZZzOrb4oQ",
        "outputId": "e1b7609e-c893-4d52-e3fd-3009c52c9ada",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [2, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_features.todense()[0].getA1())"
      ],
      "metadata": {
        "id": "3wg9nGmkb7x3",
        "outputId": "fc3b7f06-7c1d-4beb-d983-ab200e7111ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33448"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([i for i in test_features.todense()[0].getA1() if i>0])"
      ],
      "metadata": {
        "id": "kCOdDcIob_ZO",
        "outputId": "7867b68e-6e0f-493f-fc18-1206cc80556f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabVect.get_feature_names()[0:10]"
      ],
      "metadata": {
        "id": "dZu7FESrb_ce",
        "outputId": "7d76d157-31b5-4362-8b3f-12622a3b331b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['00',\n",
              " '000',\n",
              " '0000',\n",
              " '0000000004',\n",
              " '0000000005',\n",
              " '0000000667',\n",
              " '0000001200',\n",
              " '0001',\n",
              " '00014',\n",
              " '0002']"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def VectorizeAndClassify(vectorizers_list, classifiers_list):\n",
        "    for v in vectorizers_list:\n",
        "        for c in classifiers_list:\n",
        "            pipeline1 = Pipeline([(\"vectorizer\", v), (\"classifier\", c)])\n",
        "            score = cross_val_score(pipeline1, newsgroups['data'], newsgroups['target'], scoring='accuracy', cv=3).mean()\n",
        "            print('Векторизация - {}'.format(v))\n",
        "            print('Модель для классификации - {}'.format(c))\n",
        "            print('Accuracy = {}'.format(score))\n",
        "            print('===========================')"
      ],
      "metadata": {
        "id": "9R_Ov2fbb_fs"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizers_list = [CountVectorizer(vocabulary = corpusVocab)]\n",
        "classifiers_list = [LogisticRegression(C=3.0), LinearSVC(), KNeighborsClassifier()]\n",
        "VectorizeAndClassify(vectorizers_list, classifiers_list)"
      ],
      "metadata": {
        "id": "D3hkr5jdb_lk",
        "outputId": "bcd36fdf-1787-4543-b768-5fa60d2df8f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '0000000004': 3,\n",
            "                            '0000000005': 4, '0000000667': 5, '0000001200': 6,\n",
            "                            '0001': 7, '00014': 8, '0002': 9, '0003': 10,\n",
            "                            '0005111312': 11, '0005111312na1em': 12,\n",
            "                            '00072': 13, '000851': 14, '000rpm': 15,\n",
            "                            '000th': 16, '001': 17, '0010': 18, '001004': 19,\n",
            "                            '0011': 20, '001211': 21, '0013': 22, '001642': 23,\n",
            "                            '001813': 24, '002': 25, '002222': 26,\n",
            "                            '002251w': 27, '0023': 28, '002937': 29, ...})\n",
            "Модель для классификации - LogisticRegression(C=3.0)\n",
            "Accuracy = 0.937813339432037\n",
            "===========================\n",
            "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '0000000004': 3,\n",
            "                            '0000000005': 4, '0000000667': 5, '0000001200': 6,\n",
            "                            '0001': 7, '00014': 8, '0002': 9, '0003': 10,\n",
            "                            '0005111312': 11, '0005111312na1em': 12,\n",
            "                            '00072': 13, '000851': 14, '000rpm': 15,\n",
            "                            '000th': 16, '001': 17, '0010': 18, '001004': 19,\n",
            "                            '0011': 20, '001211': 21, '0013': 22, '001642': 23,\n",
            "                            '001813': 24, '002': 25, '002222': 26,\n",
            "                            '002251w': 27, '0023': 28, '002937': 29, ...})\n",
            "Модель для классификации - LinearSVC()\n",
            "Accuracy = 0.9453742497059174\n",
            "===========================\n",
            "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '0000000004': 3,\n",
            "                            '0000000005': 4, '0000000667': 5, '0000001200': 6,\n",
            "                            '0001': 7, '00014': 8, '0002': 9, '0003': 10,\n",
            "                            '0005111312': 11, '0005111312na1em': 12,\n",
            "                            '00072': 13, '000851': 14, '000rpm': 15,\n",
            "                            '000th': 16, '001': 17, '0010': 18, '001004': 19,\n",
            "                            '0011': 20, '001211': 21, '0013': 22, '001642': 23,\n",
            "                            '001813': 24, '002': 25, '002222': 26,\n",
            "                            '002251w': 27, '0023': 28, '002937': 29, ...})\n",
            "Модель для классификации - KNeighborsClassifier()\n",
            "Accuracy = 0.6655358653541747\n",
            "===========================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(newsgroups['data'], newsgroups['target'], test_size=0.5, random_state=1)"
      ],
      "metadata": {
        "id": "72N8rGicb_qD"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment(v, c):\n",
        "    model = Pipeline(\n",
        "        [(\"vectorizer\", v), \n",
        "         (\"classifier\", c)])\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print_accuracy_score_for_classes(y_test, y_pred)"
      ],
      "metadata": {
        "id": "rv1dpQqdcXgo"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment(CountVectorizer(), LinearSVC())"
      ],
      "metadata": {
        "id": "-XsUy3UjcXj0",
        "outputId": "4112334d-aee7-460a-fd34-d4b71c302cae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Метка \t Accuracy\n",
            "0 \t 0.9290322580645162\n",
            "1 \t 0.9675090252707581\n",
            "2 \t 0.9026845637583892\n",
            "3 \t 0.9245901639344263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import word2vec"
      ],
      "metadata": {
        "id": "Uk0kdbrwcXnH"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"http://rusvectores.org/static/models/rusvectores2/ruscorpora_mystem_cbow_300_2_2015.bin.gz\", \"ruscorpora_mystem_cbow_300_2_2015.bin.gz\")"
      ],
      "metadata": {
        "id": "0aK6yG96cXql",
        "outputId": "323f4bcd-c2fd-41de-9d37-7798147c08d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ruscorpora_mystem_cbow_300_2_2015.bin.gz',\n",
              " <http.client.HTTPMessage at 0x7f82f3481610>)"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'ruscorpora_mystem_cbow_300_2_2015.bin.gz'"
      ],
      "metadata": {
        "id": "tZfANwtXcXt1"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)"
      ],
      "metadata": {
        "id": "JxnIT4ojcXwq"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['холод_S', 'мороз_S', 'береза_S', 'сосна_S']\n"
      ],
      "metadata": {
        "id": "Gc7uyNBScXzz"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    if word in model:\n",
        "        print('\\nСЛОВО - {}'.format(word))\n",
        "        print('5 ближайших соседей слова:')\n",
        "        for word, sim in model.most_similar(positive=[word], topn=5):\n",
        "            print('{} => {}'.format(word, sim))\n",
        "    else:\n",
        "        print('Слово \"{}\" не найдено в модели'.format(word))"
      ],
      "metadata": {
        "id": "VlwrvdZocX3D",
        "outputId": "992875be-fd95-4e09-85b5-ed547575b6f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "СЛОВО - холод_S\n",
            "5 ближайших соседей слова:\n",
            "стужа_S => 0.7676383852958679\n",
            "сырость_S => 0.6338975429534912\n",
            "жара_S => 0.6089427471160889\n",
            "мороз_S => 0.5890367031097412\n",
            "озноб_S => 0.5776054859161377\n",
            "\n",
            "СЛОВО - мороз_S\n",
            "5 ближайших соседей слова:\n",
            "стужа_S => 0.6425479650497437\n",
            "морозец_S => 0.5947279930114746\n",
            "холод_S => 0.5890367031097412\n",
            "жара_S => 0.5522176623344421\n",
            "снегопад_S => 0.5083199143409729\n",
            "\n",
            "СЛОВО - береза_S\n",
            "5 ближайших соседей слова:\n",
            "сосна_S => 0.7943247556686401\n",
            "тополь_S => 0.7562226057052612\n",
            "дуб_S => 0.7440178394317627\n",
            "дерево_S => 0.7373415231704712\n",
            "клен_S => 0.7105200290679932\n",
            "\n",
            "СЛОВО - сосна_S\n",
            "5 ближайших соседей слова:\n",
            "береза_S => 0.7943247556686401\n",
            "дерево_S => 0.7581434845924377\n",
            "лиственница_S => 0.747814953327179\n",
            "дуб_S => 0.7412480711936951\n",
            "ель_S => 0.7363824248313904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.most_similar(positive=['холод_S', 'стужа_S'], negative=['мороз_S']))"
      ],
      "metadata": {
        "id": "BxQZHfJtcX6N",
        "outputId": "d405c7e4-b15d-40d3-9f7a-7fedddd77034",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('сырость_S', 0.5040211081504822), ('стылость_S', 0.46336129307746887), ('голод_S', 0.4604816436767578), ('зной_S', 0.45904627442359924), ('скука_S', 0.4489358067512512), ('жара_S', 0.44645121693611145), ('усталость_S', 0.4218570291996002), ('озноб_S', 0.41469818353652954), ('духота_S', 0.4099087715148926), ('неуют_S', 0.40298789739608765)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk import WordPunctTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "j_LCqeeWdGtC",
        "outputId": "38d550fe-79a1-4791-8fea-40fc014cbbe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\"rec.motorcycles\", \"rec.sport.baseball\", \"sci.electronics\",\"sci.med\"]\n",
        "newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
        "data = newsgroups['data']"
      ],
      "metadata": {
        "id": "17JMPaZhdGv9"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Подготовим корпус\n",
        "corpus = []\n",
        "stop_words = stopwords.words('english')\n",
        "tok = WordPunctTokenizer()\n",
        "for line in newsgroups['data']:\n",
        "    line1 = line.strip().lower()\n",
        "    line1 = re.sub(\"[^a-zA-Z]\",\" \", line1)\n",
        "    text_tok = tok.tokenize(line1)\n",
        "    text_tok1 = [w for w in text_tok if not w in stop_words]\n",
        "    corpus.append(text_tok1)"
      ],
      "metadata": {
        "id": "_MTMHm0_dGzi"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[:5]"
      ],
      "metadata": {
        "id": "4itoMfHKdG2U",
        "outputId": "57b1dfe1-9630-432d-d12a-6817fb688774",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['nrmendel',\n",
              "  'unix',\n",
              "  'amherst',\n",
              "  'edu',\n",
              "  'nathaniel',\n",
              "  'mendell',\n",
              "  'subject',\n",
              "  'bike',\n",
              "  'advice',\n",
              "  'organization',\n",
              "  'amherst',\n",
              "  'college',\n",
              "  'x',\n",
              "  'newsreader',\n",
              "  'tin',\n",
              "  'version',\n",
              "  'pl',\n",
              "  'lines',\n",
              "  'ummm',\n",
              "  'bikes',\n",
              "  'kx',\n",
              "  'suggest',\n",
              "  'look',\n",
              "  'zx',\n",
              "  'since',\n",
              "  'horsepower',\n",
              "  'whereas',\n",
              "  'might',\n",
              "  'bit',\n",
              "  'much',\n",
              "  'sincerely',\n",
              "  'nathaniel',\n",
              "  'zx',\n",
              "  'dod',\n",
              "  'ama'],\n",
              " ['grante',\n",
              "  'aquarius',\n",
              "  'rosemount',\n",
              "  'com',\n",
              "  'grant',\n",
              "  'edwards',\n",
              "  'subject',\n",
              "  'krillean',\n",
              "  'photography',\n",
              "  'reply',\n",
              "  'grante',\n",
              "  'aquarius',\n",
              "  'rosemount',\n",
              "  'com',\n",
              "  'grant',\n",
              "  'edwards',\n",
              "  'organization',\n",
              "  'rosemount',\n",
              "  'inc',\n",
              "  'lines',\n",
              "  'nntp',\n",
              "  'posting',\n",
              "  'host',\n",
              "  'aquarius',\n",
              "  'stgprao',\n",
              "  'st',\n",
              "  'unocal',\n",
              "  'com',\n",
              "  'richard',\n",
              "  'ottolini',\n",
              "  'writes',\n",
              "  'living',\n",
              "  'things',\n",
              "  'maintain',\n",
              "  'small',\n",
              "  'electric',\n",
              "  'fields',\n",
              "  'enhance',\n",
              "  'certain',\n",
              "  'chemical',\n",
              "  'reactions',\n",
              "  'promote',\n",
              "  'communication',\n",
              "  'states',\n",
              "  'cell',\n",
              "  'communicate',\n",
              "  'cells',\n",
              "  'nervous',\n",
              "  'system',\n",
              "  'specialized',\n",
              "  'example',\n",
              "  'perhaps',\n",
              "  'uses',\n",
              "  'true',\n",
              "  'electric',\n",
              "  'fields',\n",
              "  'change',\n",
              "  'location',\n",
              "  'time',\n",
              "  'large',\n",
              "  'organism',\n",
              "  'also',\n",
              "  'true',\n",
              "  'special',\n",
              "  'photographic',\n",
              "  'techniques',\n",
              "  'applying',\n",
              "  'external',\n",
              "  'fields',\n",
              "  'kirillian',\n",
              "  'photography',\n",
              "  'interact',\n",
              "  'fields',\n",
              "  'resistances',\n",
              "  'caused',\n",
              "  'fields',\n",
              "  'make',\n",
              "  'interesting',\n",
              "  'pictures',\n",
              "  'really',\n",
              "  'kirlian',\n",
              "  'photography',\n",
              "  'taking',\n",
              "  'pictures',\n",
              "  'corona',\n",
              "  'discharge',\n",
              "  'objects',\n",
              "  'animate',\n",
              "  'inanimate',\n",
              "  'fields',\n",
              "  'applied',\n",
              "  'objects',\n",
              "  'millions',\n",
              "  'times',\n",
              "  'larger',\n",
              "  'biologically',\n",
              "  'created',\n",
              "  'fields',\n",
              "  'want',\n",
              "  'record',\n",
              "  'biologically',\n",
              "  'created',\n",
              "  'electric',\n",
              "  'fields',\n",
              "  'got',\n",
              "  'use',\n",
              "  'low',\n",
              "  'noise',\n",
              "  'high',\n",
              "  'gain',\n",
              "  'sensors',\n",
              "  'typical',\n",
              "  'eegs',\n",
              "  'ekgs',\n",
              "  'kirlian',\n",
              "  'photography',\n",
              "  'phun',\n",
              "  'physics',\n",
              "  'type',\n",
              "  'stuff',\n",
              "  'right',\n",
              "  'soaking',\n",
              "  'chunks',\n",
              "  'extra',\n",
              "  'fine',\n",
              "  'steel',\n",
              "  'wool',\n",
              "  'liquid',\n",
              "  'oxygen',\n",
              "  'hitting',\n",
              "  'hammer',\n",
              "  'like',\n",
              "  'kirlean',\n",
              "  'setup',\n",
              "  'fun',\n",
              "  'possibly',\n",
              "  'dangerous',\n",
              "  'perhaps',\n",
              "  'pictures',\n",
              "  'diagonistic',\n",
              "  'disease',\n",
              "  'problems',\n",
              "  'organisms',\n",
              "  'better',\n",
              "  'understood',\n",
              "  'perhaps',\n",
              "  'probably',\n",
              "  'grant',\n",
              "  'edwards',\n",
              "  'yow',\n",
              "  'vote',\n",
              "  'rosemount',\n",
              "  'inc',\n",
              "  'well',\n",
              "  'tapered',\n",
              "  'half',\n",
              "  'cocked',\n",
              "  'ill',\n",
              "  'conceived',\n",
              "  'grante',\n",
              "  'aquarius',\n",
              "  'rosemount',\n",
              "  'com',\n",
              "  'tax',\n",
              "  'deferred'],\n",
              " ['liny',\n",
              "  'sun',\n",
              "  'scri',\n",
              "  'fsu',\n",
              "  'edu',\n",
              "  'nemo',\n",
              "  'subject',\n",
              "  'bates',\n",
              "  'method',\n",
              "  'myopia',\n",
              "  'reply',\n",
              "  'lin',\n",
              "  'ray',\n",
              "  'met',\n",
              "  'fsu',\n",
              "  'edu',\n",
              "  'distribution',\n",
              "  'na',\n",
              "  'organization',\n",
              "  'scri',\n",
              "  'florida',\n",
              "  'state',\n",
              "  'university',\n",
              "  'lines',\n",
              "  'bates',\n",
              "  'method',\n",
              "  'work',\n",
              "  'first',\n",
              "  'heard',\n",
              "  'newsgroup',\n",
              "  'several',\n",
              "  'years',\n",
              "  'ago',\n",
              "  'got',\n",
              "  'hold',\n",
              "  'book',\n",
              "  'improve',\n",
              "  'sight',\n",
              "  'simple',\n",
              "  'daily',\n",
              "  'drills',\n",
              "  'relaxation',\n",
              "  'margaret',\n",
              "  'corbett',\n",
              "  'authorized',\n",
              "  'instructor',\n",
              "  'bates',\n",
              "  'method',\n",
              "  'published',\n",
              "  'talks',\n",
              "  'vision',\n",
              "  'improvement',\n",
              "  'relaxation',\n",
              "  'exercise',\n",
              "  'study',\n",
              "  'whether',\n",
              "  'method',\n",
              "  'actually',\n",
              "  'works',\n",
              "  'works',\n",
              "  'actually',\n",
              "  'shortening',\n",
              "  'previously',\n",
              "  'elongated',\n",
              "  'eyeball',\n",
              "  'increasing',\n",
              "  'lens',\n",
              "  'ability',\n",
              "  'flatten',\n",
              "  'order',\n",
              "  'compensate',\n",
              "  'long',\n",
              "  'eyeball',\n",
              "  'since',\n",
              "  'myopia',\n",
              "  'result',\n",
              "  'eyeball',\n",
              "  'elongation',\n",
              "  'seems',\n",
              "  'logical',\n",
              "  'approach',\n",
              "  'correction',\n",
              "  'find',\n",
              "  'way',\n",
              "  'reverse',\n",
              "  'process',\n",
              "  'e',\n",
              "  'shorten',\n",
              "  'somehow',\n",
              "  'preferably',\n",
              "  'non',\n",
              "  'surgically',\n",
              "  'recent',\n",
              "  'studies',\n",
              "  'find',\n",
              "  'know',\n",
              "  'rk',\n",
              "  'works',\n",
              "  'changing',\n",
              "  'curvature',\n",
              "  'cornea',\n",
              "  'compensate',\n",
              "  'shape',\n",
              "  'eyeball',\n",
              "  'way',\n",
              "  'train',\n",
              "  'muscles',\n",
              "  'shorten',\n",
              "  'eyeball',\n",
              "  'back',\n",
              "  'correct',\n",
              "  'length',\n",
              "  'would',\n",
              "  'even',\n",
              "  'better',\n",
              "  'bates',\n",
              "  'idea',\n",
              "  'right',\n",
              "  'thanks',\n",
              "  'information'],\n",
              " ['mcovingt',\n",
              "  'aisun',\n",
              "  'ai',\n",
              "  'uga',\n",
              "  'edu',\n",
              "  'michael',\n",
              "  'covington',\n",
              "  'subject',\n",
              "  'buy',\n",
              "  'parts',\n",
              "  'time',\n",
              "  'nntp',\n",
              "  'posting',\n",
              "  'host',\n",
              "  'aisun',\n",
              "  'ai',\n",
              "  'uga',\n",
              "  'edu',\n",
              "  'organization',\n",
              "  'ai',\n",
              "  'programs',\n",
              "  'university',\n",
              "  'georgia',\n",
              "  'athens',\n",
              "  'lines',\n",
              "  'pricing',\n",
              "  'parts',\n",
              "  'reminds',\n",
              "  'something',\n",
              "  'chemist',\n",
              "  'said',\n",
              "  'gram',\n",
              "  'dye',\n",
              "  'costs',\n",
              "  'dollar',\n",
              "  'comes',\n",
              "  'liter',\n",
              "  'jar',\n",
              "  'also',\n",
              "  'costs',\n",
              "  'dollar',\n",
              "  'want',\n",
              "  'whole',\n",
              "  'barrel',\n",
              "  'also',\n",
              "  'costs',\n",
              "  'dollar',\n",
              "  'e',\n",
              "  'charge',\n",
              "  'almost',\n",
              "  'exclusively',\n",
              "  'packaging',\n",
              "  'delivering',\n",
              "  'chemical',\n",
              "  'particular',\n",
              "  'case',\n",
              "  'byproduct',\n",
              "  'cost',\n",
              "  'almost',\n",
              "  'nothing',\n",
              "  'intrinsically',\n",
              "  'michael',\n",
              "  'covington',\n",
              "  'associate',\n",
              "  'research',\n",
              "  'scientist',\n",
              "  'artificial',\n",
              "  'intelligence',\n",
              "  'programs',\n",
              "  'mcovingt',\n",
              "  'ai',\n",
              "  'uga',\n",
              "  'edu',\n",
              "  'university',\n",
              "  'georgia',\n",
              "  'phone',\n",
              "  'athens',\n",
              "  'georgia',\n",
              "  'u',\n",
              "  'amateur',\n",
              "  'radio',\n",
              "  'n',\n",
              "  'tmi'],\n",
              " ['tammy',\n",
              "  'vandenboom',\n",
              "  'launchpad',\n",
              "  'unc',\n",
              "  'edu',\n",
              "  'tammy',\n",
              "  'vandenboom',\n",
              "  'subject',\n",
              "  'sore',\n",
              "  'spot',\n",
              "  'testicles',\n",
              "  'nntp',\n",
              "  'posting',\n",
              "  'host',\n",
              "  'lambada',\n",
              "  'oit',\n",
              "  'unc',\n",
              "  'edu',\n",
              "  'organization',\n",
              "  'university',\n",
              "  'north',\n",
              "  'carolina',\n",
              "  'extended',\n",
              "  'bulletin',\n",
              "  'board',\n",
              "  'service',\n",
              "  'distribution',\n",
              "  'na',\n",
              "  'lines',\n",
              "  'husband',\n",
              "  'woke',\n",
              "  'three',\n",
              "  'days',\n",
              "  'ago',\n",
              "  'small',\n",
              "  'sore',\n",
              "  'spot',\n",
              "  'spot',\n",
              "  'size',\n",
              "  'nickel',\n",
              "  'one',\n",
              "  'testicles',\n",
              "  'bottom',\n",
              "  'side',\n",
              "  'knots',\n",
              "  'lumps',\n",
              "  'little',\n",
              "  'sore',\n",
              "  'spot',\n",
              "  'says',\n",
              "  'reminds',\n",
              "  'bruise',\n",
              "  'feels',\n",
              "  'recollection',\n",
              "  'hitting',\n",
              "  'anything',\n",
              "  'like',\n",
              "  'would',\n",
              "  'cause',\n",
              "  'bruise',\n",
              "  'asssures',\n",
              "  'remember',\n",
              "  'something',\n",
              "  'like',\n",
              "  'clues',\n",
              "  'might',\n",
              "  'somewhat',\n",
              "  'hypochondriac',\n",
              "  'sp',\n",
              "  'sure',\n",
              "  'gonna',\n",
              "  'die',\n",
              "  'thanks',\n",
              "  'opinions',\n",
              "  'expressed',\n",
              "  'necessarily',\n",
              "  'university',\n",
              "  'north',\n",
              "  'carolina',\n",
              "  'chapel',\n",
              "  'hill',\n",
              "  'campus',\n",
              "  'office',\n",
              "  'information',\n",
              "  'technology',\n",
              "  'experimental',\n",
              "  'bulletin',\n",
              "  'board',\n",
              "  'service',\n",
              "  'internet',\n",
              "  'launchpad',\n",
              "  'unc',\n",
              "  'edu']]"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time model_imdb = word2vec.Word2Vec(corpus, workers=4, min_count=10, window=10, sample=1e-3)"
      ],
      "metadata": {
        "id": "WIJVvSvydG5e",
        "outputId": "72a114ea-d8f2-4e7a-811b-8ba9c1d69eb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6.91 s, sys: 58.1 ms, total: 6.97 s\n",
            "Wall time: 4.72 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверим, что модель обучилась\n",
        "print(model_imdb.wv.most_similar(positive=['find'], topn=5))"
      ],
      "metadata": {
        "id": "EnnBKlt-dG8f",
        "outputId": "b712dceb-7aa4-4166-e0fd-a19155e660a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('using', 0.9875365495681763), ('etc', 0.9865429401397705), ('voltage', 0.9855812788009644), ('work', 0.9851916432380676), ('circuit', 0.9840276837348938)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_2(v, c):\n",
        "    model = Pipeline(\n",
        "        [(\"vectorizer\", v), \n",
        "         (\"classifier\", c)])\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print_accuracy_score_for_classes(y_test, y_pred)"
      ],
      "metadata": {
        "id": "rv7IpF_xdG_W"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingVectorizer(object):\n",
        "    '''\n",
        "    Для текста усредним вектора входящих в него слов\n",
        "    '''\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.size = model.vector_size\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([np.mean(\n",
        "            [self.model[w] for w in words if w in self.model] \n",
        "            or [np.zeros(self.size)], axis=0)\n",
        "            for words in X])"
      ],
      "metadata": {
        "id": "IG_S8WrMdHCm"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_score_for_classes(\n",
        "    y_true: np.ndarray, \n",
        "    y_pred: np.ndarray) -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    Вычисление метрики accuracy для каждого класса\n",
        "    y_true - истинные значения классов\n",
        "    y_pred - предсказанные значения классов\n",
        "    Возвращает словарь: ключ - метка класса, \n",
        "    значение - Accuracy для данного класса\n",
        "    \"\"\"\n",
        "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
        "    d = {'t': y_true, 'p': y_pred}\n",
        "    df = pd.DataFrame(data=d)\n",
        "    # Метки классов\n",
        "    classes = np.unique(y_true)\n",
        "    # Результирующий словарь\n",
        "    res = dict()\n",
        "    # Перебор меток классов\n",
        "    for c in classes:\n",
        "        # отфильтруем данные, которые соответствуют \n",
        "        # текущей метке класса в истинных значениях\n",
        "        temp_data_flt = df[df['t']==c]\n",
        "        # расчет accuracy для заданной метки класса\n",
        "        temp_acc = accuracy_score(\n",
        "            temp_data_flt['t'].values, \n",
        "            temp_data_flt['p'].values)\n",
        "        # сохранение результата в словарь\n",
        "        res[c] = temp_acc\n",
        "    return res\n",
        "\n",
        "def print_accuracy_score_for_classes(\n",
        "    y_true: np.ndarray, \n",
        "    y_pred: np.ndarray):\n",
        "    \"\"\"\n",
        "    Вывод метрики accuracy для каждого класса\n",
        "    \"\"\"\n",
        "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
        "    if len(accs)>0:\n",
        "        print('Метка \\t Accuracy')\n",
        "    for i in accs:\n",
        "        print('{} \\t {}'.format(i, accs[i]))"
      ],
      "metadata": {
        "id": "EgKAOE02dHFt"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучающая и тестовая выборки\n",
        "boundary = 1500\n",
        "X_train = corpus[:boundary] \n",
        "X_test = corpus[boundary:]\n",
        "y_train = newsgroups['target'][:boundary]\n",
        "y_test = newsgroups['target'][boundary:]"
      ],
      "metadata": {
        "id": "bGXO8xYwdl-Y"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_2(EmbeddingVectorizer(model_imdb.wv), LogisticRegression(C=5.0))"
      ],
      "metadata": {
        "id": "zbbzvRDndo0e",
        "outputId": "9ab4c1bb-312b-41f7-d500-0e3f034f10d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Метка \t Accuracy\n",
            "0 \t 0.8421052631578947\n",
            "1 \t 0.9320388349514563\n",
            "2 \t 0.7431192660550459\n",
            "3 \t 0.7149122807017544\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Welcome To Colaboratory",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}